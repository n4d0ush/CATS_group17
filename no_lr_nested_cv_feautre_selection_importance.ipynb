{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c23ad995-edf1-4e0d-acf3-a4dc0b577e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statistics import mean\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, \\\n",
    "    RandomizedSearchCV  # Hyperparameter tuning - GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, \\\n",
    "    KFold  # Cross-validation - StratifiedKFold, RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  # Classifier - Random Forest\n",
    "from sklearn.linear_model import LogisticRegression  # Classifier - Logistic Regression\n",
    "from sklearn.tree import DecisionTreeClassifier  # Classifier - Decision Tree\n",
    "\n",
    "from sklearn.metrics import roc_auc_score  # Evaluation metric - AUC\n",
    "from sklearn.metrics import f1_score  # Evaluation metric - F1 score\n",
    "from sklearn.metrics import accuracy_score  # Evaluation metric - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff4d8ff-6ca3-44af-bd31-2a3502ca1755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_table('Train_call.txt')  # Shows the data with samples as rows, so need to transpose\n",
    "data = data.T  # Transposes data, so that samples are now rows.\n",
    "data_target = pd.read_table('Train_clinical.txt')  # Gives sample with associated subgroup\n",
    "\n",
    "# Extract predictor and target data\n",
    "target = data_target.loc[:,\n",
    "         \"Subgroup\"]  # Isolates the subgroups from samples. We need to convert the subgroups into 0, 1, 2\n",
    "new_data_unlabeled = data.iloc[4:, :]  # This is the complete cleaned up dataset\n",
    "\n",
    "# We also need to convert the subgroups into 0, 1, 2\n",
    "for i in range(len(target)):\n",
    "    if target[i] == \"HER2+\":\n",
    "        target[i] = 0\n",
    "    elif target[i] == \"HR+\":\n",
    "        target[i] = 1\n",
    "    elif target[i] == \"Triple Neg\":\n",
    "        target[i] = 2\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_data_unlabeled, target, test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "y_train = y_train.astype(int)  # For some reason the numbers are read as strings, so convert to integers\n",
    "\n",
    "# Hyperparameter grid\n",
    "rf_hp_grid = param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 3, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "dt_hp_grid = {'max_depth': [None, 5, 20]}\n",
    "\n",
    "# CV technique for outer and inner folds\n",
    "outer_cv = StratifiedKFold(n_splits=4)\n",
    "inner_cv = StratifiedKFold(n_splits=6)\n",
    "\n",
    "# -----------\n",
    "# Plan:\n",
    "# 1) We already have a train/test split\n",
    "# 2) Cross validation: we split the training data into 4-folds: train + validation\n",
    "# 3) For each fold, we apply feature  selection to reduce the dataset\n",
    "# 4) For each fold, we apply 6-CV for hyperparameter tuning\n",
    "# 5) For each fold, we test the optimal models\n",
    "\n",
    "# Step 1) Is already done\n",
    "# Step 2) We split the data into 4 folds (outer folds)\n",
    "y_train.reset_index(drop=True, inplace=True)  # We need to reset the indices of y_train so we can apply CV split\n",
    "dt_fold_performance_lst = []  # For df and rf we will append their performances to these lists and then take the mean of these\n",
    "rf_fold_performance_lst = []\n",
    "\n",
    "dt_fold_hp_lst = []  # For df and rf we will append the best hyperparameters to these lists\n",
    "rf_fold_hp_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922fb9e6-e2b7-4543-b819-7eeaa363324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are currently in Outer fold 1\n",
      "This fold has 94 independent features\n",
      "Fitting 6 folds for each of 216 candidates, totalling 1296 fits\n",
      "Fitting 6 folds for each of 3 candidates, totalling 18 fits\n",
      "\n",
      "Random forest best hp: {'bootstrap': False, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 100} Score on validation fold: 0.65\n",
      "Decision tree best hp: {'max_depth': 5} Score on validation fold: 0.8\n",
      "Performance of random forest per fold [0.65] Average performance: 0.65\n",
      "Performance of decision tree per fold [0.8] Average performance: 0.8\n",
      "\n",
      "These are the clusters: [{2}, {3}, {5}, {38, 39}, {57, 58}, {101}, {230}, {271}, {305, 308, 302, 303}, {371}, {375}, {387, 388}, {416}, {480, 481, 482, 483, 484, 463, 465, 466, 471, 472, 473, 474, 475}, {485}, {486}, {488, 489, 487}, {491, 492, 493, 495}, {596}, {608, 609}, {610, 611}, {612, 613}, {616, 618, 619, 615}, {620, 621, 622, 623}, {665, 666, 667, 668, 669, 670, 671}, {672, 673, 674, 675, 676, 679}, {724}, {728, 729}, {730}, {734}, {803, 804, 805, 806, 814, 815, 816, 817, 818, 819, 823, 825, 826, 827, 828, 829, 830, 832, 834, 835, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854}, {899, 900, 904, 905, 906, 908, 910, 916}, {983}, {999, 1000, 1001, 1002, 1003, 1004, 1005, 1008}, {1012, 1013}, {1024, 1025, 1026, 1027, 1015, 1016, 1022}, {1032, 1030, 1031}, {1040, 1037, 1038}, {1047}, {1054}, {1064, 1062, 1063}, {1078, 1081, 1082, 1084, 1085, 1086, 1087}, {1142, 1143}, {1481, 1485, 1486, 1487}, {1489}, {1521, 1522}, {1636, 1637, 1638}, {1640, 1641, 1639}, {1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650}, {1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658}, {1659, 1660, 1661}, {1664, 1665, 1666, 1667, 1668, 1669, 1670, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1662, 1663}, {1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1692, 1693}, {1696, 1700, 1701, 1702, 1694}, {1709, 1710, 1711, 1712, 1715, 1716, 1718, 1719, 1721, 1723, 1724, 1725}, {1739, 1743, 1744, 1748, 1749, 1750, 1753, 1754, 1755, 1758, 1759}, {1769, 1770}, {1788, 1789}, {1877}, {1878, 1879}, {1891, 1897, 1899, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914}, {1919}, {2027, 2022}, {2066}, {2105, 2108}, {2113}, {2120, 2121, 2124, 2125, 2128, 2129}, {2132, 2133}, {2153, 2154, 2155, 2157, 2158, 2160, 2161}, {2165, 2166}, {2182, 2183}, {2184}, {2185}, {2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211}, {2212, 2213, 2214}, {2218}, {2219, 2220, 2221}, {2222}, {2224, 2225, 2223}, {2234, 2235, 2236, 2237}, {2242, 2243, 2244}, {2262, 2263}, {2380}, {2384, 2382}, {2723, 2725, 2726, 2729, 2730, 2732, 2733, 2734, 2737, 2751}, {2762}, {2763}, {2764, 2765}, {2766}, {2768, 2769}, {2770, 2771}, {2801}, {2802, 2805, 2806}, {2809}]\n",
      "\n",
      "For this fold, these are the top 10 important features: 2184    0.153908\n",
      "2220    0.031534\n",
      "1687    0.031180\n",
      "1663    0.025412\n",
      "2212    0.024852\n",
      "485     0.022897\n",
      "1646    0.020981\n",
      "487     0.019711\n",
      "2763    0.019613\n",
      "620     0.018535\n",
      "dtype: float64\n",
      "We are currently in Outer fold 2\n",
      "This fold has 82 independent features\n",
      "Fitting 6 folds for each of 216 candidates, totalling 1296 fits\n",
      "Fitting 6 folds for each of 3 candidates, totalling 18 fits\n",
      "\n",
      "Random forest best hp: {'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 10} Score on validation fold: 0.75\n",
      "Decision tree best hp: {'max_depth': None} Score on validation fold: 0.7\n",
      "Performance of random forest per fold [0.65, 0.75] Average performance: 0.7\n",
      "Performance of decision tree per fold [0.8, 0.7] Average performance: 0.75\n",
      "\n",
      "These are the clusters: [{5}, {32, 33, 24, 25, 26}, {56, 53, 54, 55}, {59}, {99, 69, 101, 102, 73, 74, 75, 76, 87, 88, 89, 90, 93}, {118}, {147}, {161, 163, 164, 165, 166, 167, 156, 158}, {174, 175}, {176}, {177}, {230}, {301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315}, {340, 341}, {352, 353, 354}, {365}, {375}, {384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 382, 383}, {437, 438, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484}, {485}, {488, 489, 487}, {497}, {500, 502, 503}, {618, 619, 615}, {620, 622, 623}, {716, 717, 718}, {721}, {723, 724, 726, 727, 728, 729}, {736, 737, 739, 740, 731, 735}, {743}, {758}, {790, 791}, {794}, {808, 804, 805, 806}, {813, 814, 815, 816, 817, 818, 819}, {824, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855}, {856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 872, 873, 874, 875}, {895}, {998}, {999, 1000, 1001, 1002, 1003, 1004}, {1008, 1014, 1015, 1016, 1017}, {1025, 1020, 1021, 1022}, {1057, 1061, 1055}, {1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071}, {1250, 1251, 1253}, {1288, 1289, 1290, 1293, 1294, 1295, 1296}, {1301, 1302}, {1384, 1383}, {1479, 1480, 1481, 1482, 1486}, {1597, 1598}, {1606}, {1648, 1649, 1646}, {1664, 1665, 1667, 1668, 1669, 1670, 1671, 1672, 1656, 1662, 1663}, {1674, 1675, 1676, 1677, 1678, 1679, 1681}, {1686, 1687, 1688, 1690, 1693}, {1701, 1694}, {1739, 1748}, {1973}, {2024, 2023}, {2025}, {2026, 2027}, {2056}, {2063, 2064, 2065, 2068, 2069, 2070, 2071, 2074, 2075, 2076, 2077, 2078}, {2154, 2155, 2157, 2158, 2159, 2160, 2161, 2162, 2163}, {2184}, {2209, 2210, 2211, 2192, 2193, 2194, 2195, 2196, 2202, 2203, 2204, 2205, 2206, 2207}, {2219, 2212, 2213, 2214}, {2247}, {2411}, {2468, 2469, 2471, 2472, 2473, 2480}, {2481}, {2482, 2483}, {2490, 2485}, {2496, 2497, 2498, 2491, 2492, 2493}, {2500, 2501, 2502}, {2656}, {2669, 2661, 2662, 2663}, {2723, 2724, 2725, 2726, 2727, 2729, 2730, 2731, 2732, 2736, 2737, 2738, 2739, 2740, 2741, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759}, {2762}, {2763}, {2764, 2765}, {2770, 2771}]\n",
      "\n",
      "For this fold, these are the top 10 important features: 2184    0.211162\n",
      "390     0.050025\n",
      "303     0.036167\n",
      "1067    0.034062\n",
      "2656    0.030981\n",
      "1295    0.030881\n",
      "620     0.030756\n",
      "1973    0.028559\n",
      "497     0.028043\n",
      "147     0.026741\n",
      "dtype: float64\n",
      "We are currently in Outer fold 3\n",
      "This fold has 94 independent features\n",
      "Fitting 6 folds for each of 216 candidates, totalling 1296 fits\n",
      "Fitting 6 folds for each of 3 candidates, totalling 18 fits\n",
      "\n",
      "Random forest best hp: {'bootstrap': True, 'max_depth': 5, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} Score on validation fold: 0.6\n",
      "Decision tree best hp: {'max_depth': None} Score on validation fold: 0.75\n",
      "Performance of random forest per fold [0.65, 0.75, 0.6] Average performance: 0.6666666666666666\n",
      "Performance of decision tree per fold [0.8, 0.7, 0.75] Average performance: 0.75\n",
      "\n",
      "These are the clusters: [{2}, {3, 4, 5, 6, 8, 9, 18}, {21, 22, 23, 24, 25, 26}, {53, 54, 56, 57, 58}, {62}, {65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 63}, {138, 139}, {145, 147, 148}, {160, 161, 162, 165, 166, 167, 168, 169, 170, 171, 156, 157, 158, 159}, {174, 175}, {176}, {178}, {207}, {230}, {305, 303}, {385, 386, 387, 388}, {436}, {441, 442}, {455, 459, 460, 461, 462, 463, 464, 465, 466, 467, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484}, {485}, {488, 489, 487}, {618, 619}, {620, 622, 623}, {697, 694}, {723, 724, 725}, {726}, {728}, {730}, {736, 740, 731, 732, 733, 734, 735}, {743, 744, 745, 746, 747, 748, 749}, {751, 755, 756, 757, 758, 759}, {762, 763, 764}, {765}, {768, 769, 771, 772, 773, 775, 776, 783, 784, 789, 794, 798, 803, 804, 805, 815, 818, 766, 767}, {825}, {832, 834, 835, 836, 839, 840, 842, 844, 845, 847, 848, 849, 850, 851, 852, 853, 854, 855}, {856, 874, 861, 863}, {899, 900, 909, 910, 914, 916}, {941}, {983}, {998}, {999, 1000, 1001, 1002, 1003, 1004, 1005, 1008}, {1009}, {1011}, {1012, 1013}, {1025, 1026, 1027, 1028, 1029, 1015, 1016}, {1059, 1061, 1055}, {1064, 1065, 1066, 1067, 1068, 1069}, {1071, 1072, 1073, 1074, 1075, 1076, 1078}, {1089, 1084, 1085, 1086}, {1111}, {1288, 1289, 1293, 1294, 1295, 1296}, {1305, 1301}, {1306}, {1648, 1645, 1646, 1647}, {1677, 1678, 1663}, {1743}, {1909, 1910}, {1973}, {2016, 2017, 2018, 2019, 2020, 2013, 2014, 2015}, {2021}, {2024, 2022, 2023}, {2025}, {2026, 2027, 2028, 2029, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040}, {2069, 2070}, {2105, 2108}, {2112, 2113, 2116, 2118, 2109, 2110, 2111}, {2124, 2125, 2126, 2127}, {2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161}, {2163, 2164}, {2168, 2165, 2166, 2167}, {2184}, {2192, 2194, 2195, 2196, 2199, 2202, 2203, 2204}, {2208, 2209, 2210, 2211, 2206, 2207}, {2212, 2213, 2214}, {2218}, {2219, 2220, 2221, 2223, 2224, 2225}, {2229, 2230, 2231}, {2234, 2236}, {2242, 2243, 2244, 2245, 2246, 2247}, {2264}, {2272, 2265}, {2288, 2290, 2292, 2293}, {2392, 2391}, {2401}, {2411}, {2439, 2431}, {2662}, {2675, 2676, 2677, 2678, 2679}, {2688, 2689, 2690, 2693, 2694, 2695, 2696, 2697, 2698, 2682, 2683, 2685, 2686, 2687}, {2730, 2732, 2736, 2737, 2739, 2740, 2741, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757}, {2760}, {2763}, {2824, 2832}]\n",
      "\n",
      "For this fold, these are the top 10 important features: 2184    0.104145\n",
      "2036    0.031885\n",
      "2019    0.026550\n",
      "726     0.024265\n",
      "622     0.022493\n",
      "69      0.021452\n",
      "1306    0.020750\n",
      "2021    0.020057\n",
      "736     0.019828\n",
      "2236    0.019237\n",
      "dtype: float64\n",
      "We are currently in Outer fold 4\n",
      "This fold has 81 independent features\n",
      "Fitting 6 folds for each of 216 candidates, totalling 1296 fits\n",
      "Fitting 6 folds for each of 3 candidates, totalling 18 fits\n",
      "\n",
      "Random forest best hp: {'bootstrap': False, 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10} Score on validation fold: 0.85\n",
      "Decision tree best hp: {'max_depth': 5} Score on validation fold: 0.85\n",
      "Performance of random forest per fold [0.65, 0.75, 0.6, 0.85] Average performance: 0.7125\n",
      "Performance of decision tree per fold [0.8, 0.7, 0.75, 0.85] Average performance: 0.775\n",
      "\n",
      "These are the clusters: [{12}, {14, 15}, {16, 17}, {32}, {308}, {354}, {474}, {480}, {484}, {485}, {488, 489, 487}, {490, 491, 492, 499, 500}, {553, 554}, {555, 558}, {594}, {610}, {612}, {616, 618, 619, 615}, {620, 621, 622, 623}, {674}, {679}, {686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 700}, {718}, {723, 724, 725}, {728, 729, 726, 727}, {736, 737, 738, 739, 740, 731, 732, 733, 734, 735}, {743}, {769, 770, 771, 772, 773, 755, 757, 758, 759}, {839, 840, 841, 842, 843, 844, 848, 849, 850, 851, 852, 853, 854}, {874}, {998}, {1000, 1001, 999}, {1101, 1103, 1104, 1105, 1106}, {1281}, {1296, 1293, 1294, 1295}, {1384, 1391, 1382, 1383}, {1424, 1425, 1422, 1423}, {1593, 1595, 1597, 1598}, {1606}, {1635, 1636}, {1638}, {1640, 1641, 1639}, {1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650}, {1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658}, {1659, 1660, 1661}, {1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1662, 1663}, {1682}, {1683, 1684, 1685, 1686, 1687, 1688, 1690, 1691, 1692, 1693}, {1696, 1701, 1694}, {1862}, {1877}, {1878, 1879}, {1891, 1894, 1895, 1896, 1897, 1899, 1900}, {1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914}, {1952, 1953, 1949, 1950, 1951}, {1962, 1963}, {1973}, {2047}, {2056}, {2154}, {2168}, {2183}, {2184}, {2185}, {2208, 2209, 2210, 2211, 2193, 2194, 2195, 2196, 2199, 2202, 2203, 2204, 2205, 2206, 2207}, {2212, 2213, 2214, 2219, 2220, 2221, 2223, 2224}, {2226}, {2241}, {2281}, {2293, 2291, 2285}, {2379}, {2661, 2662, 2663}, {2709}, {2721, 2722}, {2723, 2724, 2725, 2726, 2727, 2730, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2748, 2751}, {2760}, {2763}, {2764, 2765}, {2773, 2774}, {2819, 2820, 2821, 2822, 2823, 2824, 2830, 2831, 2832}, {2833}]\n",
      "\n",
      "For this fold, these are the top 10 important features: 2184    0.290796\n",
      "2206    0.068520\n",
      "1900    0.036236\n",
      "2662    0.035191\n",
      "723     0.034777\n",
      "698     0.032648\n",
      "1646    0.031128\n",
      "489     0.026313\n",
      "743     0.024060\n",
      "1879    0.023539\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "    print(f\"We are currently in Outer fold {i + 1}\")\n",
    "    X_train_fold = X_train.iloc[train_index,\n",
    "                   :]  # train_index is a list of indices, but we can pass lists of indices in np\n",
    "    y_train_fold = y_train[train_index]\n",
    "    X_validate_fold = X_train.iloc[test_index, :]\n",
    "    y_validate_fold = y_train[test_index]\n",
    "\n",
    "    # Step 3) Feature selection\n",
    "\n",
    "    # Combining X_train and y_train again in one dataframe\n",
    "    y_train_df = pd.DataFrame(y_train_fold)\n",
    "    train_data = X_train_fold.copy()\n",
    "    train_data['Subgroup'] = list(y_train_df.iloc[:, 0])\n",
    "\n",
    "    # Perform Chi-Squared test for each feature (column)\n",
    "    chi2_results = []\n",
    "    target_variable = 'Subgroup'\n",
    "\n",
    "    # For each feature, find correlation with target variable\n",
    "    for feature in train_data.columns:\n",
    "        if feature != target_variable:  # Check if the feature is categorical\n",
    "            contingency_table = pd.crosstab(train_data[feature], train_data[target_variable])\n",
    "            results = chi2_contingency(contingency_table)\n",
    "            chi2_results.append({'Feature': feature, 'Chi-Squared': results[0], 'p-value': results[1]})\n",
    "\n",
    "    df_chi = pd.DataFrame(chi2_results)  # Save the results into a dataframe\n",
    "\n",
    "    # Filter out the features with insignificant target correlations\n",
    "    significant = []\n",
    "    for j in range(0, len(df_chi['p-value'])):\n",
    "        if df_chi.iloc[j, 2] <= 0.05:  # when p-value is smaller than or equal to critical value\n",
    "            significant.append(True)\n",
    "        else:\n",
    "            significant.append(False)\n",
    "\n",
    "    # Append whether feature is significant (True/False) to chi dataframe\n",
    "    df_chi['significant'] = significant\n",
    "\n",
    "    # Select the significant features\n",
    "    significant_features = df_chi[df_chi['significant']]\n",
    "    features = pd.DataFrame(significant_features['Feature'])\n",
    "    features_list = list(features['Feature'])\n",
    "\n",
    "    # Make for each feature an empty set\n",
    "    clusters_list = [set() for i in range(len(features_list) + 1)]\n",
    "    k = 0\n",
    "\n",
    "    # Go through each significant feature\n",
    "    for j in range(len(features_list) - 1):\n",
    "        # Retrieve feature information\n",
    "        feature_j_data = X_train.iloc[:, features_list[j]]\n",
    "        feature_neighbour_data = X_train.iloc[:, features_list[j + 1]]\n",
    "        current_cluster = clusters_list[k]\n",
    "\n",
    "        # If cluster is empty, add current feature to it\n",
    "        if len(current_cluster) == 0:\n",
    "            current_cluster.add(features_list[j])\n",
    "\n",
    "        # If neighboring significant feature has high correlation, add to current cluster. Else, create new cluster\n",
    "        if abs(feature_j_data.corr(feature_neighbour_data)) > 0.8:\n",
    "            # print(\"Feature\",features_list[j],\"and feature\",features_list[j+1], \"are correlated\")\n",
    "            current_cluster.add(features_list[j + 1])\n",
    "        else:\n",
    "            # print(\"Feature\",features_list[j],\"and feature\", features_list[j+1], \"are not correlated\")\n",
    "            k += 1  # we need to go to a new cluster\n",
    "\n",
    "            # If the final feature does not correlate, we still need to create its own cluster\n",
    "            if features_list[j + 1] == features_list[-1]:\n",
    "                current_cluster = clusters_list[k]\n",
    "                current_cluster.add(features_list[j + 1])\n",
    "\n",
    "    # Remove all the empty clusters from clusters_list\n",
    "    clusters_list = [cluster for cluster in clusters_list if cluster != set()]\n",
    "\n",
    "    # from each cluster we randomly pick one feature\n",
    "    indep_features_list = []\n",
    "    for cluster in clusters_list:\n",
    "        feature_random = random.choice(list(cluster))\n",
    "        indep_features_list.append(feature_random)\n",
    "\n",
    "    print('This fold has', len(indep_features_list), 'independent features')\n",
    "    # select the X_train_fold data for only independent features\n",
    "    r_X_train_fold = X_train_fold.iloc[:, indep_features_list]\n",
    "    r_X_validate_fold = X_validate_fold.iloc[:, indep_features_list]\n",
    "\n",
    "    # Step 4) Apply 5CV Grid search to each X_train_fold\n",
    "\n",
    "    # Define classifiers\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\n",
    "    dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "    # Define grid search object for all classifiers\n",
    "    # USE ACCURACY AS A PLACEHOLDER MEASURE!!! (to be removed)\n",
    "    rf_grid_search = GridSearchCV(estimator=rf_classifier, param_grid=rf_hp_grid, cv=inner_cv, scoring='accuracy',\n",
    "                                  verbose=1)\n",
    "    dt_grid_search = GridSearchCV(estimator=dt_classifier, param_grid=dt_hp_grid, cv=inner_cv, scoring='accuracy',\n",
    "                                  verbose=1)\n",
    "\n",
    "    # Run grid search for all classifiers on the training data of this current fold\n",
    "    rf_grid_search.fit(r_X_train_fold, y_train_fold)\n",
    "    dt_grid_search.fit(r_X_train_fold, y_train_fold)\n",
    "\n",
    "    # Extract the most important parameter and the corresponding score\n",
    "    rf_best_hp = rf_grid_search.best_params_\n",
    "    rf_best_score = rf_grid_search.best_score_\n",
    "\n",
    "    dt_best_hp = dt_grid_search.best_params_\n",
    "    dt_best_score = dt_grid_search.best_score_\n",
    "\n",
    "    # store the best hyperparameters in the dictionaries\n",
    "    rf_fold_hp_lst.append([rf_best_hp, rf_best_score])\n",
    "    dt_fold_hp_lst.append([dt_best_hp, dt_best_score])\n",
    "\n",
    "    # Define new models with the optimal hyperparameters\n",
    "    best_rf_classifier = rf_grid_search.best_estimator_\n",
    "    best_dt_classifier = dt_grid_search.best_estimator_\n",
    "\n",
    "    # Step 5) Now that we have the best models for this fold, we can test the performance on X_train_fold\n",
    "    rf_y_predict_fold = best_rf_classifier.predict(r_X_validate_fold)\n",
    "    dt_y_predict_fold = best_dt_classifier.predict(r_X_validate_fold)\n",
    "\n",
    "    # Retrieve the accuracy score\n",
    "    rf_accuracy = accuracy_score(rf_y_predict_fold, y_validate_fold)\n",
    "    dt_accuracy = accuracy_score(dt_y_predict_fold, y_validate_fold)\n",
    "\n",
    "    print(\"\\nRandom forest best hp:\", rf_best_hp, \"Score on validation fold:\", rf_accuracy)\n",
    "    print(\"Decision tree best hp:\", dt_best_hp, \"Score on validation fold:\", dt_accuracy)\n",
    "\n",
    "    # Append performance to list\n",
    "    rf_fold_performance_lst.append(rf_accuracy)\n",
    "    dt_fold_performance_lst.append(dt_accuracy)\n",
    "\n",
    "    print(\"Performance of random forest per fold\", rf_fold_performance_lst, \"Average performance:\",\n",
    "          mean(rf_fold_performance_lst))\n",
    "    print(\"Performance of decision tree per fold\", dt_fold_performance_lst, \"Average performance:\",\n",
    "          mean(dt_fold_performance_lst))\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Feature selection for this current fold:\n",
    "    # Print cluster (to map features to)\n",
    "    print(\"\\nThese are the clusters:\", clusters_list)\n",
    "\n",
    "    # Retrieve feature importances and names of features\n",
    "    importance = best_rf_classifier.feature_importances_\n",
    "    names_features = r_X_train_fold.columns\n",
    "\n",
    "    # plot feature importance\n",
    "    forest_importances = pd.Series(importance, index=names_features)\n",
    "    sort_forest_importances = forest_importances.sort_values(ascending=False)\n",
    "    top_forest_importances = sort_forest_importances[:10]\n",
    "\n",
    "    print(\"\\nFor this fold, these are the top 10 important features:\")\n",
    "    print(top_forest_importances)\n",
    "\n",
    "    # ----------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
